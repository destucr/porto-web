---
title: "Telly: BISINDO Sign Language Learning"
description: A native iOS app teaching Indonesian Sign Language (BISINDO) through real-time gesture recognition**.
image: /images/telly-thumbnail.webp
tags:
- iOS
- SwiftUI
- Create ML
- SwiftData
githubUrl: https://github.com/destucikal
appStoreUrl: ""
videoUrl: /images/telly-demo.mp4
screenshots:
- /images/telly-splashscreen.webp
- /images/telly-onboarding-welcome.webp
- /images/telly-onboarding-inputname.webp
- /images/telly-onboarding-howtouse.webp
- /images/telly-onboarding-guide.webp
- /images/telly-home-course.webp
- /images/telly-home-course-modalguide.webp
- /images/telly-course-start.webp
---

#### What it does
Users learn **BISINDO** signs by watching demonstrations, then practicing with their device's camera. The app uses
**on-device machine learning** to verify their hand shapes and provide **instant feedback**.

#### How I built it
Trained **Create ML models** to recognize 50+ common **BISINDO** signs using a **dataset** I compiled from video
demonstrations. The models run entirely **on-device** for privacy and speed—no internet required after download.

Built the interface in **SwiftUI** with a **course structure** that unlocks new lessons as users demonstrate
proficiency. The camera feed shows **real-time overlay feedback** when the model detects their hand position matches the
target sign.

Used **SwiftData** to track progress **locally**, storing completion rates and accuracy scores for each sign.

#### Challenges
The hardest part was getting enough **training data**—**BISINDO** resources are scarce compared to ASL. I had to film
and **annotate** my own reference videos, then **augment** the dataset with **rotations** and **lighting variations** to
make the model robust to different environments.

Balancing **model accuracy** with **file size** was tricky since the entire model ships with the app.